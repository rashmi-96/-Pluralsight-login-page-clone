{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLp11jjtyBJM4YgwMxFUp2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashmi-96/-Pluralsight-login-page-clone/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LnE6K4jUt8Mo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "def load_vectors(fname):\n",
        "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "    data = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        data[tokens[0]] = map(float, tokens[1:])\n",
        "    return data"
      ],
      "metadata": {
        "id": "OxKFM2j9CPXl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = {\n",
        "    \"bert_emotion\": \"bert_emo/best-model.pt\",\n",
        "    \"bert_info\": \"bert_info/best-model.pt\",\n",
        "    \"ftx_emotion\": \"fasttext_emo/model-en.bin\",\n",
        "    \"ftx_info\": \"fasttext_info/model-en.bin\"\n",
        "}\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "9U6uTIsACUna"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_fasttext_model(model_path):\n",
        "    # Load FastText model\n",
        "    load_vectors\n",
        "    pass\n",
        "\n",
        "fasttext_emotion_model = load_fasttext_model(PATH[\"ftx_emotion\"])\n",
        "fasttext_info_model = load_fasttext_model(PATH[\"ftx_info\"])\n",
        "\n",
        "Device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "T7HDx584KXFb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_emotion(bert_value, ftx_value):\n",
        "    emotion_proba = [0, 0, 0, 0]\n",
        "\n",
        "    # Add BERT emotion predictions\n",
        "    for i in range(len(bert_value)):\n",
        "        emotion_proba[i] += bert_value[i]\n",
        "\n",
        "    # Add FastText emotion predictions\n",
        "    for i in range(len(ftx_value[0])):\n",
        "        key = ftx_value[0][i].split('_')[4]\n",
        "        if key == \"sad\":\n",
        "            emotion_proba[0] += ftx_value[1][i]\n",
        "        elif key == \"fear\":\n",
        "            emotion_proba[1] += ftx_value[1][i]\n",
        "        elif key == \"anger\":\n",
        "            emotion_proba[2] += ftx_value[1][i]\n",
        "        else:\n",
        "            emotion_proba[3] += ftx_value[1][i]\n",
        "\n",
        "    # Normalize probabilities\n",
        "    for i in range(len(emotion_proba)):\n",
        "        emotion_proba[i] = emotion_proba[i] / 2\n",
        "\n",
        "    return emotion_proba"
      ],
      "metadata": {
        "id": "CEIVlQSKKd6Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_info(bert_value, ftx_value):\n",
        "    info_proba = [0, 0]\n",
        "\n",
        "    # Add BERT informativeness predictions\n",
        "    if bert_value[0] >= 0.5:\n",
        "        info_proba[1] += bert_value[0]\n",
        "    else:\n",
        "        info_proba[0] += bert_value[0]\n",
        "\n",
        "    # Add FastText informativeness predictions\n",
        "    for i in range(len(ftx_value[0])):\n",
        "        key = ftx_value[0][i].split('_')[4]\n",
        "        if key == \"INFORMATIVE\":\n",
        "            info_proba[1] += ftx_value[1][i]\n",
        "        else:\n",
        "            info_proba[0] += ftx_value[1][i]\n",
        "\n",
        "    # Normalize probabilities\n",
        "    for i in range(len(info_proba)):\n",
        "        info_proba[i] = info_proba[i] / 2\n",
        "\n",
        "    return info_proba"
      ],
      "metadata": {
        "id": "l26bcwQmKmpr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVMsBlb8KoGS",
        "outputId": "61661a42-8202-4a0c-9923-2eb09bc79eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/68.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/68.8 kB\u001b[0m \u001b[31m931.5 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m739.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n",
            "Building wheels for collected packages: fasttext\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_emotion(model,tokenizer,sentence)\n",
        "\tmodel.eval()\n",
        "\n",
        "\ttokanized_inputs = tokenizer.batch_encode_plus(\n",
        "\t[sentence],\n",
        "\tmax_length=256,\n",
        "\tpadding='max_length',\n",
        "\ttruncation= True,\n",
        "\treturn_tensors=\"pt\"\n",
        "\t)\n",
        "\n",
        "\tsource_id = tokenized_inputs[\"inputs_ids\"]\n",
        "\tprediction = F.softmax(model(source_id.to(device)), dim=1)\n",
        "\n",
        "\tclass_preds = torch.argmax(prediction, dim=1)\n",
        "\n",
        "\tif class_preds == 0:\n",
        "\t\treturn \"sad\", torch.max(prediction), prediction\n",
        "\telif class_preds == 1:\n",
        "\t\treturn \"Fear\", torch.max(prediction), prediction\n",
        "\telif class_preds == 2:\n",
        "\t\treturn \"Anger\", torch.max(prediction), prediction\n",
        "\telse:\n",
        "\t\treturn \"Joy\", torch.max(prediction),prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "pcEyVSEMzt-e",
        "outputId": "22a7596b-76b2-47da-9fa5-0e59125d0d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "expected ':' (<ipython-input-8-a2f50b0599ad>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-a2f50b0599ad>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def predict_emotion(model,tokenizer,sentence)\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_info(model, tokenizer, sentence):\n",
        "\tmodel.eval()\n",
        "\n",
        "\ttokenized_inputs = tokenizer.batch_encode_plus(\n",
        "\t[sentence],\n",
        "\tmax_length=256,\n",
        "\tpadding='max_length',\n",
        "\ttruncation =True,\n",
        "\treturn_tensors=\"pt\"\n",
        "\t)\n",
        "\n",
        "\tsource_id = tokenized_inputs[\"inputs_ids\"]\n",
        "\tprediction = torch.sigmoid(model(source_id.to(device)))\n",
        "\n",
        "\tif prediction.item() >:\n",
        "\t\treturn \"Informative\", prediction\n",
        "\telse:\n",
        "\t\treturn \"Non-Informative\", prediction"
      ],
      "metadata": {
        "id": "aAOwEuTezx8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess the text, make predictions, ensemble the predictions, return all results\n",
        "\n",
        "def sample_predict(text):\n",
        "\tpreprocessed_text = text_[reprocessor.pipeline(sentence=text)['state'][0]\n",
        "\n",
        "\tb_emotion, _, value = predict_emotion(bert_emotion, tokrnizer, preprocessed_text)\n",
        "\tbert_emotion_value = value.cpi().detach().numpy()[0]\n",
        "\n",
        "\n",
        "\tb_info, _ = predict_info(bert_info, tokenizer, preprocessed_text)\n",
        "\tbert_info_value = value.cpu().detech().numpy()[0]\n",
        "\n",
        "\tftx_emotion_value = ftx_emotion.predict(preprocessed_text, k=4)\n",
        "\tftx_info_value = ftx_info.predict(preprocessed_text, k=2)\n",
        "\n",
        "\tf_emotion = ftx_emotion_value[0][0].split('_')[4]\n",
        "\tf_info = ftx_info_value[0][0].split('_')[4]\n",
        "\n",
        "\temotion_proba = ensemble_emotion(bert_emotion_value, ftx_emotion_value)\n",
        "\tinfo_proba = ensemble(bert_info,ftx_info_value)"
      ],
      "metadata": {
        "id": "htV1cJrMz1vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = np.argmax(emotion_proba)\n",
        "if key == 0:\n",
        "\tens_emotion = \"sad\"\n",
        "elif key == 1:\n",
        "\tens_emotion = \"Fear\"\n",
        "elif key == 2:\n",
        "\tens_emotion = \"Anger\"\n",
        "else:\n",
        "\tens_emotion = \"Joy\"\n",
        "\n",
        "key = np.argmax(infor_proba)\n",
        "if key == 0;\n",
        "\tens_info = \"Non-informative\"\n",
        "else:\n",
        "\tens_info = \"Informative\"\n",
        "\n",
        "return preprocessed_text, b_emotions, be_info, f_emotion, f_info, ens_emotion, ens_info"
      ],
      "metadata": {
        "id": "LgnUst_Y0BQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_batch(filename):\n",
        "\tdf = pd.read_csv(filename)\n",
        "\n",
        "\ti = 0\n",
        "\trow_list =[]\n",
        "\tlength = str(len(df))\n",
        "\tfor index,row in df.iterrows():\n",
        "\t\tpreprocessed_text,bert_emotion,bert_info, ftx_emotion, ftx_info, ens_emotion, ens_info = sampele\n",
        "\t\ttemp ={\n",
        "\t\t\t'date': row.get('created_at', date.today().isoformat()),\n",
        "\t\t\t'original_text': row['Text']\n",
        "\t\t\t'preprocessed_text': preprocessed_text,\n",
        "\t\t\t'original_label': row['Label'],\n",
        "\t\t\t'bert_emotion': bert_emotion,\n",
        "\t\t\t'bert_info': bert_info,\n",
        "\t\t\t'ftx_emotion': ftx_emotion,\n",
        "\t\t\t'ftx_info': ftx_info,\n",
        "\t\t\t'ens_emotion':ens_emotion,\n",
        "\t\t\t'ens_info': ens_info,\n",
        "\t\t}\n",
        "\t\trow_list.append(temp)\n",
        "\t\ti += 1\n",
        "\t\tprint(\"Processing: \" str(i) + \"out of \" + length + \"tweets\")\n",
        "\tdf_temp = pd.DataFrame(row_list)"
      ],
      "metadata": {
        "id": "3vjyl-dx0GXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(text):\n",
        "\ttext_preprocessor = TextPreprocessing()\n",
        "\tpreprocessed_text = text_preprocessor.pipeline(sentence =text)['state'][0]\n",
        "\n",
        "\ttokenizer, bert_emotion, bert_info = config_bert_models()\n",
        "\n",
        "\t#BERT emotions\n",
        "\t_, _, value = predict_emotion(bert_emotion, tokenizer, preprocessed_text)\n",
        "\tbert_emotion_value = value.cpu().detach().numpy()[0]\n",
        "\n",
        "\t#BERT Info\n",
        "\t_, value = predict_info(bert_info, tokenizer, preprocessed_text)\n",
        "\tbert_info_value = value.cpu().detach().numpy()[0]\n",
        "\n",
        "\t#Fasttext models\n",
        "\tftx_emotion = fasttext.load_model(PATH['ftx_emotion'])\n",
        "\tftx_info = fasttext.load_model(PATH['ftx_info'])\n",
        "\n",
        "\tftx_emotion_value = ftx_emption.predict(preprocessed_text, k=4)\n",
        "\tftx_info_value = ftx_info.predict(preprocessed_text, k=4)\n",
        "\n",
        "#ensemble\n",
        "\temotion_proba = ensemble_emotion(bert_emotion_value, ftx_emotion_value)\n",
        "\tinfo_proba = ensemble_info(bert_info_value, ftx_info_value)"
      ],
      "metadata": {
        "id": "IrIxR8u40KsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import fasttext\n",
        "from datetime import date"
      ],
      "metadata": {
        "id": "QLEqA8VLLJsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "y-N_ugnjLOLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction functions\n",
        "def predict_emotion(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    tokenized_inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
        "    input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = tokenized_inputs[\"attention_mask\"].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        probs = F.softmax(outputs.logits, dim=1)\n",
        "    return probs"
      ],
      "metadata": {
        "id": "7vikwHAKLkNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_info(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    tokenized_inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
        "    input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = tokenized_inputs[\"attention_mask\"].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        probs = torch.sigmoid(outputs.logits)\n",
        "    return probs"
      ],
      "metadata": {
        "id": "S6ZA878VL079"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Ensemble prediction\n",
        "def ensemble_predictions(bert_probs, ftx_probs):\n",
        "    # Dummy implementation: averaging probabilities (adjust according to actual needs)\n",
        "    return (bert_probs.cpu().numpy() + ftx_probs) / 2"
      ],
      "metadata": {
        "id": "jCrzae6TL2ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch processing function\n",
        "def process_tweets(filename, output_file):\n",
        "    tokenizer, bert_emotion, bert_info, ftx_emotion, ftx_info = load_fasttext_model()\n",
        "    df = pd.read_csv('preprocessed_tweets.csv')\n",
        "    results = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        text = df(row['clean_text'])\n",
        "        bert_emotion_probs = predict_emotion(bert_emotion, tokenizer, text)\n",
        "        bert_info_prob = predict_info(bert_info, tokenizer, text)\n",
        "        ftx_emotion_prob = ftx_emotion.predict(text)[0]\n",
        "        ftx_info_prob = ftx_info.predict(text)[0]\n",
        "\n",
        "        emotion_probs = ensemble_predictions(bert_emotion_probs, ftx_emotion_prob)\n",
        "        info_prob = ensemble_predictions(bert_info_prob, ftx_info_prob)\n",
        "\n",
        "        # Decode predictions to labels, etc.\n",
        "        result = {\n",
        "            \"original_text\": row['Text'],\n",
        "            \"processed_text\": text,\n",
        "            \"emotion\": np.argmax(emotion_probs),\n",
        "            \"info\": \"Informative\" if info_prob > 0.5 else \"Non-Informative\"\n",
        "        }\n",
        "        results.append(result)\n",
        "        print(f\"Processed: {text} - Emotion: {result['emotion']} - Info: {result['info']}\")\n",
        "\n",
        "    pd.DataFrame(results).to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "7rukw5vsMIrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    input_file = \"preprocessed_tweets.csv\"\n",
        "    output_file = \"processed_tweets.csv\"\n",
        "    process_tweets(input_file, output_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "GjOZ1CPzPE6L",
        "outputId": "1fc87cad-2e7e-4f84-bc95-f2ed61e9f082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "load_fasttext_model() missing 1 required positional argument: 'model_path'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-44c8ec946e55>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-44c8ec946e55>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"preprocessed_tweets.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"processed_tweets.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprocess_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-05f9d4ba501d>\u001b[0m in \u001b[0;36mprocess_tweets\u001b[0;34m(filename, output_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Batch processing function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_emotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mftx_emotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mftx_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_fasttext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preprocessed_tweets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: load_fasttext_model() missing 1 required positional argument: 'model_path'"
          ]
        }
      ]
    }
  ]
}